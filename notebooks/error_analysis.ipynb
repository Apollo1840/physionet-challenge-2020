{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dmitriishubin/Desktop\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### insert the name of the last run\n",
    "!tensorboard --logdir=runs/May11_01-21-42_dmitrii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import ast\n",
    "import json\n",
    "import os\n",
    "\n",
    "from metrics import Metric\n",
    "from utils.KPI_plots import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "competition_metric = Metric()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG_PATH = './data/CV_debug/A/'\n",
    "DATA_PATH = './data/A/formatted/'\n",
    "\n",
    "list_records = [i[:-4] for i in os.listdir(DEBUG_PATH) if i.find('.npy')!=-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_waveform(data_path,name):\n",
    "    \n",
    "    signal = np.load(data_path+name+'.npy')\n",
    "        \n",
    "    return signal\n",
    "\n",
    "def load_label(data_path,name):\n",
    "    \n",
    "    label = json.load(open(data_path+name+'.npy'))\n",
    "        \n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main processing pipeline\n",
    "scores_errors = np.array([])\n",
    "scores_competition = np.array([])\n",
    "records = np.array([]).astype(np.str)\n",
    "labels = np.array([])\n",
    "preds = np.array([])\n",
    "\n",
    "#load all records\n",
    "for record in list_records:\n",
    "    \n",
    "    label = load_label(DATA_PATH,record)\n",
    "    records = np.append(records,label['filename'])\n",
    "    label = label['labels_training_merged']\n",
    "    \n",
    "    preds = load_label(DEBUG_PATH,record)\n",
    "    preds = preds['predicted_label']\n",
    "    \n",
    "    #calc score for each record\n",
    "    scores_competition = np.append(scores_competition,competition_metric.compute(labels, preds))\n",
    "    scores_errors = np.append(scores_errors,np.sum(np.abs(np.array(label) - np.array(preds))))\n",
    "    \n",
    "    #add predictions and labels for overall KPI estimation\n",
    "    labels = np.append(labels,np.array(label))\n",
    "    preds = np.append(preds,np.array(pred))\n",
    "\n",
    "#plot modified consustion matrix\n",
    "matrix = competition_metric.compute_modified_confusion_matrix(labels, preds)\n",
    "Labal_names = ['IAVB'\n",
    "'AF',\n",
    "'AFL',\n",
    "'Brady',\n",
    "'CRBBB',\n",
    "'IRBBB',\n",
    "'LAnFB',\n",
    "'LAD',\n",
    "'LBBB',\n",
    "'LQRSV',\n",
    "'NSIVCB',\n",
    "'PR',\n",
    "'PAC',\n",
    "'PVC',\n",
    "'LPR',\n",
    "'LQT',\n",
    "'QAb',\n",
    "'RAD',\n",
    "'RBBB',\n",
    "'SA',\n",
    "'SB',\n",
    "'SNR',\n",
    "'STach',\n",
    "'SVPB',\n",
    "'TAb',\n",
    "'TInv',\n",
    "'VP'\n",
    " ]\n",
    "\n",
    "\n",
    "plot_confusion_matrix(matrix,Labal_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keeep records with errors only\n",
    "records = records[np.where(scores_errors > 0)]\n",
    "scores_errors = scores_errors[np.where(scores_errors > 0)]\n",
    "\n",
    "#TODO: clarify the type of sort\n",
    "score_order = np.argsort(scores_errors)\n",
    "records = records[score_order[::1]]\n",
    "\n",
    "#save a list of files in csv\n",
    "pd.DataFrame(records).to_csv('./data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_record(record,data):\n",
    "    \n",
    "    ecg = np.load(DATA_PATH+record+'.npy')\n",
    "    ecg = preprocessing.run(ecg)\n",
    "    meta = json.load(open(DATA_PATH+ f'{record}.json'))\n",
    "    \n",
    "    heatmap = np.array(data[record]['heatmap'],dtype=np.float)\n",
    "    \n",
    "    #plot the data\n",
    "    fig,ax = plt.subplots(figsize=(20,20))\n",
    "    fig.suptitle(record+', labels: '+str(meta['labels']))\n",
    "    for i in range(12):\n",
    "        ax.plot(ecg[:,i]-2.5*i)\n",
    "    ax.plot(heatmap-2.5*12)\n",
    "    plt.show()\n",
    "    \n",
    "    return 0\n",
    "\n",
    "plot_record(record=records[0],data=data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
